{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"glowscript\" class=\"glowscript\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "if (typeof Jupyter !== \"undefined\") { window.__context = { glowscript_container: $(\"#glowscript\").removeAttr(\"id\")};}else{ element.textContent = ' ';}",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for displaying DataFrames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import supplementary visualization code visuals.py from project root folder\n",
    "import vpython as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'winequality-red.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m red \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mwinequality-red.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m;\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m white \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mwinequality-white.csv\u001b[39m\u001b[39m'\u001b[39m, low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'winequality-red.csv'"
     ]
    }
   ],
   "source": [
    "red = pd.read_csv('winequality-red.csv', low_memory=False, sep=';')\n",
    "white = pd.read_csv('winequality-white.csv', low_memory=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the features in the data-set and their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last column quality is a metric of how good a specific wine was rated to be. For our purposes, let's consider all wines with ratings 7 and above to be of very good quality, wines with 5 and 6 to be of average quality, and wines less than 5 to be of insipid quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wines = data.shape[0]\n",
    "\n",
    "# Number of wines with quality rating above 6\n",
    "quality_above_6 = data.loc[(data['quality'] > 6)]\n",
    "n_above_6 = quality_above_6.shape[0]\n",
    "\n",
    "# Number of wines with quality rating below 5\n",
    "quality_below_5 = data.loc[(data['quality'] < 5)]\n",
    "n_below_5 = quality_below_5.shape[0]\n",
    "\n",
    "# Number of wines with quality rating between 5 to 6\n",
    "quality_between_5 = data.loc[(data['quality']) >= 5 & (data['quality'] <= 6)]\n",
    "n_between_5 = quality_between_5.shape[0]\n",
    "\n",
    "# Percentage of wines with quality rating above 6\n",
    "greater_percent = n_above_6*100/n_wines\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of wine data: {}\".format(n_wines))\n",
    "print(\"Wines with rating 7 and above: {}\".format(n_above_6))\n",
    "print(\"Wines with rating less than 5: {}\".format(n_below_5))\n",
    "print(\"Wines with rating 5 and 6: {}\".format(n_between_5))\n",
    "print(\"Percentage of wines with quality 7 and above: {:.2f}%\".format(greater_percent))\n",
    "\n",
    "# Some more additional data analysis\n",
    "display(np.round(data.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell block to see the distributions on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize skewed continuous features of original data\n",
    "vs.distribution(data, \"quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Exploring Relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data, alpha = 0.3, figsize = (40,40), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data.corr()\n",
    "#display(correlation)\n",
    "plt.figure(figsize=(14, 12))\n",
    "heatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe containing only pH and fixed acidity columns to visualize their co-relations\n",
    "fixedAcidity_pH = data[['pH', 'fixed acidity']]\n",
    "\n",
    "#Initialize a joint-grid with the dataframe, using seaborn library\n",
    "gridA = sns.JointGrid(x=\"fixed acidity\", y=\"pH\", data=fixedAcidity_pH, size=6)\n",
    "\n",
    "#Draws a regression plot in the grid \n",
    "gridA = gridA.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\n",
    "\n",
    "#Draws a distribution plot in the same grid\n",
    "gridA = gridA.plot_marginals(sns.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedAcidity_citricAcid = data[['citric acid', 'fixed acidity']]\n",
    "g = sns.JointGrid(x=\"fixed acidity\", y=\"citric acid\", data=fixedAcidity_citricAcid, size=6)\n",
    "g = g.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\n",
    "g = g.plot_marginals(sns.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedAcidity_density = data[['density', 'fixed acidity']]\n",
    "gridB = sns.JointGrid(x=\"fixed acidity\", y=\"density\", data=fixedAcidity_density, size=6)\n",
    "gridB = gridB.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\n",
    "gridB = gridB.plot_marginals(sns.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatileAcidity_quality = data[['quality', 'volatile acidity']]\n",
    "g = sns.JointGrid(x=\"volatile acidity\", y=\"quality\", data=volatileAcidity_quality, size=6)\n",
    "g = g.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\n",
    "g = g.plot_marginals(sns.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can visualize relationships of discreet values better with a bar plot\n",
    "\n",
    "fig, axs = plt.subplots(ncols=1,figsize=(10,6))\n",
    "sns.barplot(x='quality', y='volatile acidity', data=volatileAcidity_quality, ax=axs)\n",
    "plt.title('quality VS volatile acidity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_alcohol = data[['alcohol', 'quality']]\n",
    "\n",
    "g = sns.JointGrid(x=\"alcohol\", y=\"quality\", data=quality_alcohol, size=6)\n",
    "g = g.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\n",
    "g = g.plot_marginals(sns.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1,figsize=(10,6))\n",
    "sns.barplot(x='quality', y='alcohol', data=quality_alcohol, ax=axs)\n",
    "plt.title('quality VS alcohol')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Select any two features of your choice and view their relationship\n",
    " featureA = 'pH'\n",
    " featureB = 'alcohol'\n",
    " featureA_featureB = data[[featureA, featureB]]\n",
    "\n",
    " g = sns.JointGrid(x=featureA, y=featureB, data=featureA_featureB, size=6)\n",
    " g = g.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\n",
    " g = g.plot_marginals(sns.distplot)\n",
    "\n",
    " fig, axs = plt.subplots(ncols=1,figsize=(10,6))\n",
    " sns.barplot(x=featureA, y=featureB, data=featureA_featureB, ax=axs)\n",
    " plt.title('quality VS alcohol')\n",
    "\n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature find the data points with extreme high or low values\n",
    "for feature in data.keys():\n",
    "    \n",
    "    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(data[feature], q=25)\n",
    "    \n",
    "    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(data[feature], q=75)\n",
    "    \n",
    "    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    interquartile_range = Q3 - Q1\n",
    "    step = 1.5 * interquartile_range\n",
    "    \n",
    "    # Display the outliers\n",
    "    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "    display(data[~((data[feature] >= Q1 - step) & (data[feature] <= Q3 + step))])\n",
    "    \n",
    "# OPTIONAL: Select the indices for data points you wish to remove\n",
    "outliers  = []\n",
    "\n",
    "# Remove the outliers, if any were specified\n",
    "good_data = data.drop(data.index[outliers]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Using Machine Learning to Predict the Quality of Wines Data Preparation: \n",
    "### First, we'll apply some transforms to convert our regression problem into a classification problem. Then, we'll use our data to create feature-set and target labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the splits for categories. 1-4 will be poor quality, 5-6 will be average, 7-10 will be great\n",
    "bins = [1,4,6,10]\n",
    "\n",
    "#0 for low quality, 1 for average, 2 for great quality\n",
    "quality_labels=[0,1,2]\n",
    "data['quality_categorical'] = pd.cut(data['quality'], bins=bins, labels=quality_labels, include_lowest=True)\n",
    "\n",
    "#Displays the first 2 columns\n",
    "display(data.head(n=2))\n",
    "\n",
    "# Split the data into features and target label\n",
    "quality_raw = data['quality_categorical']\n",
    "features_raw = data.drop(['quality', 'quality_categorical'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we will write a function that will accept a ML algorithm of our choice, and use our data to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import two classification metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_predict_evaluate(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "#    inputs:\n",
    "#       - learner: the learning algorithm to be trained and predicted on\n",
    "#       - sample_size: the size of samples (number) to be drawn from training set\n",
    "#       - X_train: features training set\n",
    "#       - y_train: quality training set\n",
    "#       - X_test: features testing set\n",
    "#       - y_test: quality testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    \"\"\"\n",
    "    Fit/train the learner to the training data using slicing with 'sample_size' \n",
    "    using .fit(training_features[:], training_labels[:])\n",
    "    \"\"\"\n",
    "    start = time() # Get start time of training\n",
    "    learner = learner.fit(X_train[:sample_size], y_train[:sample_size]) #Train the model\n",
    "    end = time() # Get end time of training\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the predictions on the first 300 training samples(X_train), \n",
    "    and also predictions on the test set(X_test) using .predict()\n",
    "    \"\"\"\n",
    "    start = time() # Get start time\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    \n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # Compute accuracy on the first 300 trainin samples which is y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n",
    "        \n",
    "    # Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # Compute F1-score on the the first 300 training samples using fbeta_score()\n",
    "    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=0.5, average='micro')\n",
    "        \n",
    "    # Compute F1-score on the test set which is y_test\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5, average='micro')\n",
    "\n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any three supervised learning classification models from sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the three models\n",
    "clf_A = GaussianNB()\n",
    "clf_B = DecisionTreeClassifier(max_depth=None, random_state=None)\n",
    "clf_C = RandomForestClassifier(max_depth=None, random_state=None)\n",
    "\n",
    "\n",
    "# Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "# HINT: samples_100 is the entire training set i.e. len(y_train)\n",
    "# HINT: samples_10 is 10% of samples_100\n",
    "# HINT: samples_1 is 1% of samples_100\n",
    "\n",
    "samples_100 = len(y_train)\n",
    "samples_10 = int(len(y_train)*10/100)\n",
    "samples_1 = int(len(y_train)*1/100)\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict_evaluate(clf, samples, X_train, y_train, X_test, y_test)\n",
    "\n",
    "#print(results)\n",
    "\n",
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "vs.visualize_classification_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation - Extracting Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a supervised learning model that has 'feature_importances_'\n",
    "model = RandomForestClassifier(max_depth=None, random_state=None)\n",
    "\n",
    "# Train the supervised model on the training set using .fit(X_train, y_train)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the feature importances using .feature_importances_ \n",
    "importances = model.feature_importances_\n",
    "\n",
    "print(X_train.columns)\n",
    "print(importances)\n",
    "\n",
    "# Plot\n",
    "vs.feature_plot(importances, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = RandomForestClassifier(max_depth=None, random_state=None)\n",
    "\n",
    "# Create the parameters or base_estimators list you wish to tune, using a dictionary if needed.\n",
    "# Example: parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\n",
    "\n",
    "\"\"\"\n",
    "n_estimators: Number of trees in the forest\n",
    "max_features: The number of features to consider when looking for the best split\n",
    "max_depth: The maximum depth of the tree\n",
    "\"\"\"\n",
    "parameters = {'n_estimators': [10, 20, 30], 'max_features':[3,4,5, None], 'max_depth': [5,6,7, None]}\n",
    "\n",
    "# TODO: Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(fbeta_score, beta=0.5, average=\"micro\")\n",
    "\n",
    "# TODO: Perform grid search on the claszsifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5, average=\"micro\")))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(best_clf)\n",
    "print(\"\\nFinal accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5,  average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, you can test out your model by giving it a bunch of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Give inputs in this order: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide,\n",
    "total sulfur dioxide, density, pH, sulphates, alcohol\n",
    "\n",
    "\"\"\"\n",
    "wine_data = [[8, 0.2, 0.16, 1.8, 0.065, 3, 16, 0.9962, 3.42, 0.92, 9.5],\n",
    "            [8, 0, 0.16, 1.8, 0.065, 3, 16, 0.9962, 3.42, 0.92, 1 ],\n",
    "            [7.4, 2, 0.00, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 0.6]]\n",
    "            \n",
    "# Show predictions\n",
    "for i, quality in enumerate(best_clf.predict(wine_data)):\n",
    "    print(\"Predicted quality for Wine {} is: {}\".format(i+1, quality))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
